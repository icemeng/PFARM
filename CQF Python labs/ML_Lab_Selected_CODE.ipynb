{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/frasagui/CQF_June_2020/blob/master/Copy_of_ML_Lab_Selected_CODE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BY08oameLiHB"
   },
   "source": [
    "# Machine Learning Lab I - Supervised Learning\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>Workshop by Dr Richard Diamond </center>\n",
    "\n",
    "<center> London, 16 April 2020 </center>\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "<center> <h3>Selected Python Code</h3> </center>\n",
    "\n",
    "**This file contains selected code for Classifier fitting and useful graphical illustrations from ML Lab I. Experimental dev and repetitive code was removed.** \n",
    "\n",
    "------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5Pe8fxcLiHC"
   },
   "source": [
    "# Resources\n",
    "\n",
    "\n",
    "scikit-learn [User Guide](http://scikit-learn.org/stable/user_guide.html) is your first point of call\n",
    "---------\n",
    "\n",
    "* Generalised Linear Models\n",
    "http://scikit-learn.org/stable/modules/linear_model.html\n",
    "\n",
    "* Support Vector Machines \n",
    "http://scikit-learn.org/stable/modules/svm.html\n",
    "\n",
    "* Ensemble Methods\n",
    "http://scikit-learn.org/stable/modules/ensemble.html#adaboost\n",
    "\n",
    "\n",
    "also recommended from online resources\n",
    "---------\n",
    "\n",
    "* [Quick walk-through](https://www.kdnuggets.com/2018/02/tour-top-10-algorithms-machine-learning-newbies.html) with simplifying plots: Logistic Regression, LDA, Decision Trees, Naive Bayes, K-Nearest Neighbors, LVQ, SVM, Bagging and Random Forest, Boosting and AdaBoost -- those are our **keywords**.\n",
    "<br><br>\n",
    "\n",
    "* **Explore** the code and vignetters from [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/)** plus Youtube videos from the author, Jake VanderPlas. _Online Textbook_\n",
    "<br><br> \n",
    "\n",
    "* Certain articles from towardsdatascience.com on [Logistic Regression](https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8), also [Crossvalidation](https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6). \n",
    "<br><br> \n",
    "\n",
    "\n",
    "paper textbooks\n",
    "---------\n",
    "\n",
    "*  _Machine Learning: An Applied Mathematics Introduction_, our introduction from Paul Wilmott, 2019. \n",
    "\n",
    "Look into the maths of various kinds of 'distances' between observations. Also understand why quite a few ML techniques (KNN, Decision Trees) might not represent any 'learning'.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "* _Big Data and Machine Learning in Quantitative Investment_ by Tony Guida. A collection of chapters -- each shows different application in finance, to the varied degree of clarity (no code). \n",
    "<br><br> \n",
    "\n",
    "\n",
    "* _An Introduction to Statistical Learning with Applications in R_ by James/Witten/Hastie/Tibshirani [book website](http://faculty.marshall.usc.edu/gareth-james/ISL/) and [pdf](http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf) available from the authors.\n",
    "<br><br> \n",
    "\n",
    "\n",
    "\n",
    "Slidepacks from Data Science and Machine Learning I, II modules lectures, labs and remember you will have access to Electives, which will also have ML content.    I think you should not overlook our own Python notebooks that will come with ML Lab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6LMCnVZyLiHD"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#import pandas_datareader as pdr\n",
    "#from pandas_datareader import data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "id": "24zmb2j3LiHG",
    "outputId": "faf466bd-b666-4075-89b7-35963618fdd7"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-fc742b35e5a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'downloads/Corporate_PD.xlsm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mxl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msheet_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine)\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstringify_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/excel/_xlrd.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Install xlrd >= 1.0.0 for Excel support\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"xlrd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/excel/_xlrd.py\u001b[0m in \u001b[0;36mload_workbook\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xlrd/__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[0;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_contents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpeek\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb\"PK\\x03\\x04\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# a ZIP file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'downloads/Corporate_PD.xlsm'"
     ]
    }
   ],
   "source": [
    "xl = pd.ExcelFile('data/Corporate_PD.xlsm') \n",
    "xl.sheet_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tTWuzzj7LiHJ"
   },
   "outputs": [],
   "source": [
    "AltmanScores_names = [['WC/TA', 'Working Capital/TA'], ['RE/TA', 'Retained Earnings/TA'], ['EBIT/TA', 'EBIT/TA'], ['ME/TL', 'Market Cap/Total Liabilities'], ['S/TA', 'Sales/TA'], ]\n",
    "#also ['Const','Intercept'] # scores_altman[0][0]\n",
    "\n",
    "# usecols stopped working after pandas 0.20.0\n",
    "# this has been fixed but NOT IN pandas 0.23.0 install\n",
    "# https://github.com/pandas-dev/pandas/issues/18273\n",
    "# https://github.com/jacksonjos/pandas/commit/e25710065c8b9291a35289bf3ef97cc62bf966cf\n",
    "\n",
    "#Y_Response = pd.read_excel(xl, sheet_name=\"Logit\", header=0, usecols=['Default'])\n",
    "#X_Features = pd.read_excel(xl, sheet_name=\"Logit\", header=0, usecols=['WC/TA', 'RE/TA', 'EBIT/TA', 'ME/TL', 'S/TA'])\n",
    "#['Const', WC/TA', 'RE/TA', 'EBIT/TA', 'ME/TL', 'S/TA']\n",
    "\n",
    "X_Features = pd.read_excel(xl, sheet_name=\"Logit\", usecols=[4,5,6,7,8])\n",
    "X_Features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e4g_KDybLiHM",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_Features.info() #len(X_Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vMP46QKSLiHO",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_Response = pd.read_excel(xl, sheet_name='Logit', usecols=[2])\n",
    "Y_Response.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZCGwKWKoLiHR"
   },
   "outputs": [],
   "source": [
    "Y_Response = Y_Response['Default'].ravel()\n",
    "print(Y_Response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dMlZ4804LiHT"
   },
   "outputs": [],
   "source": [
    "Y_Response_2D = np.reshape(Y_Response, (-1, 1)) # For the fussy OLS computation in sklearn IMPORTANT FEATURE\n",
    "print(Y_Response_2D)\n",
    "\n",
    "# Reshaping an array from 1D plain list (above) to 2D is IMPORTANT functionality. Similar to vec2mat() in Matlab\n",
    "#x = np.reshape(x, (len(x),-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6HlEuM4LiHW"
   },
   "source": [
    "## Chapter. Logistic Regression as Classifier\n",
    "\n",
    "### Default/survival classification (corporate PD)\n",
    "\n",
    "--------\n",
    "\n",
    "For our corporate defaults, **logistic regression** computed in Excel by explicit MLE (based on the log of logit function) has the following exact result:\n",
    "<br><br>\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\text{Default} = + 0.4146 \\,\\text{WC/TA} -1.454 \\,\\text{RE/TA} -8.00 \\,\\text{EBIT/TA} -1.5936 \\,\\text{ME/TL} + 0.6198 \\,\\text{S/TA}+\\varepsilon_t\n",
    "\\end{equation*}\n",
    "<br><br>\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G6vs0lvGLiHX",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "# sklearn Logistic Regression as the classifier.  Does not need explicit intercept.\n",
    "logit = linear_model.LogisticRegression(C=1e5)\n",
    "logit.fit(X_Features, Y_Response)  ###FITTING DONE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P3kIQHzQLiHZ"
   },
   "outputs": [],
   "source": [
    "print()\n",
    "print('Logit beta coefficients for WC/TA, RE/TA, EBIT/TA, ME/TL, S/TA:')\n",
    "print(logit.coef_)\n",
    "print()\n",
    "print('Intercept:')\n",
    "print(logit.intercept_)\n",
    "print()\n",
    "print('model.classes_ =', logit.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y2634z-cLiHb"
   },
   "source": [
    "#### Sigmoid Plotting\n",
    "\n",
    "The long routine below is coded just to produce a plot: data from each feature is plotted vs **{0,1}** value of Y (vertically).\n",
    "\n",
    "The sigmoid function is plotted for the range of RE/TA values $-7, 5$. \n",
    "\n",
    "np.linspace(X_min, X_max, 3000) generates 3000 values in that range incrementally -- those are our input values (axe X)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9_VMBy0fLiHc"
   },
   "outputs": [],
   "source": [
    "def logistic_sigmoid(xb):\n",
    "    return (1 / (1 + np.exp(-xb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L0oV4DFbLiHe"
   },
   "outputs": [],
   "source": [
    "#AltmanScores_names = [['WC/TA', 'Working Capital/TA'], ['RE/TA', 'Retained Earnings/TA'], ['EBIT/TA', 'EBIT/TA'], ['ME/TL', 'Market Cap/Total Liabilities'], ['S/TA', 'Sales/TA'], ]\n",
    "\n",
    "\n",
    "#Procedure RELIES X_Features, Y_Response variables to be existing\n",
    "def logistic_plot(X_min, X_max, FeatureName, FeatureBetaIdx):  \n",
    "    \n",
    "    plt.clf() #clears the figure drawing space, nothing to do with classifier!\n",
    "    fig, ax = plt.subplots(figsize=(18,10))  #fig = plt.figure(figsize=(18,10))\n",
    "        \n",
    "    # 1. Plot two clusters of observations at Y={0,1} on a scatter\n",
    "    ax.scatter(X_Features[FeatureName], Y_Response, c=Y_Response, zorder=20) #X_Features[FeatureName].ravel()\n",
    "    \n",
    "    # 2. Plot CALIBRATED sigmoid function (in orange) -- with the coeffient from Logistic Regression\n",
    "    X_Sim = np.linspace(X_min, X_max, 3000) #Fill in 3,000 values for the range of axe X\n",
    "    Y_Loss = logistic_sigmoid(X_Sim * logit.coef_[0,FeatureBetaIdx] + logit.intercept_) # Y_Loss = logistic_sigmoid(X_Sim * logit.coef_[0,FeatureBetaIdx] + logit.intercept_).ravel() \n",
    "    ax.plot(X_Sim, Y_Loss, color='red', linewidth=3)\n",
    "    \n",
    "    # 3. Below plots OLS line\n",
    "    ols = linear_model.LinearRegression()\n",
    "    ols.fit(X_Features, Y_Response_2D) ### Fitting done here. Fussy OLS computation requires np.reshape(Y_Response, (-1, 1))\n",
    "    \n",
    "    ax.plot(X_Sim, ols.coef_[0,FeatureBetaIdx] * X_Sim + ols.intercept_, linewidth=1) #we plot beta*x + intercept (omitting other betas)\n",
    "    ax.axhline(.5, color='.5')\n",
    "    \n",
    "    plt.ylabel('Default Indicator 1 or 0', fontsize=22) # also ax.set_ylabel('Default Indicator')\n",
    "    plt.xlabel('wrt Feature: ' + AltmanScores_names[FeatureBetaIdx][1], fontsize=22)\n",
    "    plt.xticks(range(X_min, X_max), fontsize=14) #Axe X range\n",
    "    plt.yticks([0, 1], fontsize=14)\n",
    "    plt.ylim(-.25, 1.25)\n",
    "    plt.xlim(X_min, X_max) #Axe X range\n",
    "    plt.legend(('Logistic Regression', 'Linear Regression'),\n",
    "           loc=\"lower right\", fontsize=14)\n",
    "    #plt.show()\n",
    "    return ax\n",
    "\n",
    "# plt.subplots() is a function that returns a tuple containing a figure and axes object(s). \n",
    "# Thus when using fig, ax = plt.subplots() you unpack this tuple into the variables fig and ax. \n",
    "#type(fig) #<class 'matplotlib.figure.Figure'>\n",
    "#type(ax) #<class 'matplotlib.axes._subplots.AxesSubplot'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EkWbZCnJLiHh"
   },
   "outputs": [],
   "source": [
    "logistic_plot(-7, 5, 'RE/TA', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zm0rxbgyLiHj"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFOJjPEXLiHj"
   },
   "source": [
    "#### Transition Probabilities\n",
    "\n",
    "This functionality is present in EACH classifier, and here it predicts Probability Survival (first column) and Probability Default (second column). This is essentially our **transition matrix** by observation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ygaxsYulLiHk"
   },
   "outputs": [],
   "source": [
    "# Remember we fitted the Logistic Model (no need to specify an intercept additionaly)\n",
    "#logit = linear_model.LogisticRegression(C=1e5)\n",
    "#logit.fit(X_Features, Y_Response)\n",
    "\n",
    "#clf.predict(X_Features)\n",
    "logit.predict_proba(X_Features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Af_EzprQLiHm"
   },
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9JH0ifPDLiHm"
   },
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html\n",
    "from sklearn.naive_bayes import BernoulliNB  #MultinomialNB\n",
    "\n",
    "# Run Classification \n",
    "naivebayes = BernoulliNB()\n",
    "naivebayes.fit(X_Features, Y_Response)  ###FITTING DONE HERE\n",
    "\n",
    "naivebayes.get_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fAAoAuj5LiHp"
   },
   "source": [
    "#### Transition Probabilities -  from Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6DlwcB_qLiHp"
   },
   "outputs": [],
   "source": [
    "naivebayes.predict_proba(X_Features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OL8vprtaLiHr"
   },
   "source": [
    "\n",
    "-----\n",
    "\n",
    "## Chapter. Crossvalidation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BPzmPH88LiHs"
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "X_Train, X_Test, Y_Train, Y_Test = model_selection.train_test_split(X_Features, Y_Response, test_size=0.5, shuffle=True)\n",
    "X_Train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "miRGWC77LiHu"
   },
   "outputs": [],
   "source": [
    "X_Test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EdT0nFANLiHw"
   },
   "outputs": [],
   "source": [
    "logit2 = linear_model.LogisticRegression(C=1e5) #Re-estimate on Training Dataset\n",
    "logit2.fit(X_Train, Y_Train)  ###FITTING DONE HERE\n",
    "\n",
    "Y_Pred = logit2.predict(X_Test)\n",
    "\n",
    "print('Coefficients: ')\n",
    "print( logit2.coef_.ravel())\n",
    "\n",
    "print('Population Coefficients: ')\n",
    "print( logit.coef_.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d5ci10ohLiHz"
   },
   "outputs": [],
   "source": [
    "logit2.score(X_Test, Y_Test) #Classification accuracy for train/test dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CbIxO3iTLiH1"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Y_Test, Y_Pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S6ZS_GEgLiH3"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(Y_Test, Y_Pred)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bp62Av-tLiH5"
   },
   "outputs": [],
   "source": [
    "#  function for plotting confusion matrix\n",
    "def plot_cm(cm, target_names, title,\n",
    "            cmap=plt.cm.Greens):\n",
    "    cm_norm = cm * 1. / cm.sum(axis=1)[:, np.newaxis]  # standardize the confusion matrix\n",
    "    plt.imshow(cm_norm, interpolation='nearest', cmap=cmap)\n",
    "    plt.colorbar()\n",
    "    ticks = np.arange(len(target_names))\n",
    "    plt.xticks(ticks, target_names, rotation=45)\n",
    "    plt.yticks(ticks, target_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    \n",
    "plt.clf\n",
    "plt.figure(figsize=(20, 5), facecolor='w')\n",
    "plt.subplot(111)\n",
    "\n",
    "plot_cm(confusion_matrix, ['Survival','Default'], 'Confusion Matrix - Logistic Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GG325wKLiH7"
   },
   "source": [
    "#### Confusion Matrix (visualised above)\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yf4KLHp7LiH7"
   },
   "source": [
    "#### Area under ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LTa4pGO4LiH8"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nGDebT_ZLiH-"
   },
   "outputs": [],
   "source": [
    "def plot_logistic_ROC(default_ind):\n",
    "\n",
    "    default_ind_name = ['Positive (default)', 'Positive (survival)'] # This is in terms of label for Positives for Roc Curve\n",
    "    \n",
    "    logit_roc_aucT = roc_auc_score(Y_Test, logit2.predict(X_Test)) #Here, logit2 can run on Test or repeated on Train\n",
    "    fprT, tprT, thresholdsT = roc_curve(Y_Test, logit2.predict_proba(X_Test)[:,1], pos_label=default_ind)\n",
    "\n",
    "    logit_roc_aucP = roc_auc_score(Y_Response, logit.predict(X_Features))\n",
    "    fprP, tprP, thresholdsP = roc_curve(Y_Response, logit.predict_proba(X_Features)[:,1], pos_label=default_ind)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,8))  #fig = plt.figure(figsize=(18,10))\n",
    "    \n",
    "    # 1) Plot a diagnoal line of fully random classifier\n",
    "    ax.plot([0, 1], [0, 1],'r--', label='Random Classifier')\n",
    "    \n",
    "    # 1) Plot ROC Curve for the precictions on Test Dataset\n",
    "    ax.plot(fprT, tprT, label='Train/Test Regression (area = %0.2f)' % logit_roc_aucT)\n",
    "    \n",
    "    # 1) Plot ROC Curve for the Full Dataset (Population)\n",
    "    ax.plot(fprP, tprP, label='Population Regression (area = %0.2f)' % logit_roc_aucP)\n",
    "    \n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False ' + default_ind_name[default_ind] + ' Rate', fontsize=20)\n",
    "    plt.ylabel('True ' + default_ind_name[default_ind] + ' Rate', fontsize=20)\n",
    "    plt.legend(loc=\"lower right\", fontsize=14)\n",
    "    plt.show()\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9s9ZcoDVLiIB"
   },
   "outputs": [],
   "source": [
    "plot_logistic_ROC(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yKL_NozLiID"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Chapter 5. Imbalanced Samples\n",
    "\n",
    "### Bagging Regressor\n",
    "\n",
    "\n",
    "There exists Python packages to address [imbalanced samples](https://github.com/scikit-learn-contrib/imbalanced-learn). **HOWEVER** the situation of impalanced sample calls for Bagging Classifier pipeline (which is over Logit Classifier) and Ensemble Methods as an alternative classifier.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mqcTbcp6LiID"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h4h98h0nLiIF"
   },
   "outputs": [],
   "source": [
    "#We are going to use Logistic Classifier. Below were already executed so 'logit' variable exists\n",
    "#logit = linear_model.LogisticRegression(C=1e5)\n",
    "#logit.fit(X_Features, Y_Response)\n",
    "\n",
    "bagging_reg_logit = BaggingRegressor(logit)\n",
    "bagging_reg_logit.fit(X_Features, Y_Response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rcN2xpACLiIH"
   },
   "outputs": [],
   "source": [
    "bagging_pred_logit = bagging_reg_logit.predict(X_Features) #this is just an array with predictions\n",
    "bagging_pred_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B7wYySU-LiIM"
   },
   "outputs": [],
   "source": [
    "print()\n",
    "print('Returns the coefficient of determination R^2 of the prediction:')\n",
    "print(bagging_reg_logit.score(X_Features, Y_Response, sample_weight=None))\n",
    "print('NOTE. This is for exact prediction but we have likelihood.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQvbqe7ILiIO"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,10))\n",
    "plt.scatter(X_Features['RE/TA'], bagging_pred_logit, c=bagging_pred_logit, zorder=20) #X_Features[FeatureName].ravel()\n",
    "plt.colorbar()\n",
    "plt.ylabel('Default Likelihood: Bagged Logistic Classifier', fontsize=20) # also ax.set_ylabel('Default Indicator')\n",
    "plt.xlabel('wrt Feature: ' + 'RE/TA', fontsize=20)\n",
    "\n",
    "X_Sim = np.linspace(-4, 2, 1500) #Fill in 3,000 values for the range of axe\n",
    "Y_Loss = logistic_sigmoid(X_Sim * logit.coef_[0,1] + logit.intercept_) # Y_Loss = logistic_sigmoid(X_Sim * logit.coef_[0,FeatureBetaIdx] + logit.intercept_).ravel() \n",
    "plt.plot(X_Sim, Y_Loss, color='red', linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRpXEUDOLiIQ"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Chapter. Support Vector Machines (clustering vs prediction) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4JYCEFKWLiIQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YQ3V3sHHLiIS"
   },
   "outputs": [],
   "source": [
    "# Run Support Vector Classification \n",
    "C=1e5\n",
    "SVM_SVC = SVC(C=C, probability=True)\n",
    "SVM_SVC.fit(X_Features, Y_Response) ###FITTING DONE HERE\n",
    "\n",
    "SVM_SVC.get_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxNozz25LiIV"
   },
   "source": [
    "#### Transition Probabilities: now from Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QN4A0-FRLiIV"
   },
   "outputs": [],
   "source": [
    "SVM_SVC.predict_proba(X_Features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LKSEiKTeLiIY"
   },
   "source": [
    "-----\n",
    "\n",
    "#### Support Vector illustration (limited to two features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g5sGvnOOLiIY"
   },
   "outputs": [],
   "source": [
    "#Let's leave **TWO FEATURES** X_Features_2D -- essentially Restricted model M0 from Corporate PD.xls formulation.\n",
    "\n",
    "X_Features_2D = X_Features.loc[:,['RE/TA', 'ME/TL']] \n",
    "Y_Response = pd.read_excel(xl, sheet_name='Logit', usecols=[2]) \n",
    "#we reload and Y_Response as DataFrame for plotting -- even if classifier 'doesn't like' it but its output the same\n",
    "\n",
    "#type(Y_Response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4kqk0cQULiIa"
   },
   "outputs": [],
   "source": [
    "C=1e5 #To soften the margins try much smaller values, such as C=1\n",
    "SVM_2D = SVC(C=C, probability=True) #kernel='linear'\n",
    "SVM_2D.fit(X_Features_2D, Y_Response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1KrWUbszLiIc"
   },
   "outputs": [],
   "source": [
    "# Plot the decision function for a 2D SVC\n",
    "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    xlim = (-3, 3) #Correction to see the entire decision boundary for SVM_2D based on X_Features.loc[:,['RE/TA', 'ME/TL']]\n",
    "    \n",
    "    # create grid to evaluate model\n",
    "    x = np.linspace(xlim[0], xlim[1], 30)\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = model.decision_function(xy).reshape(X.shape)\n",
    "    \n",
    "    # plot decision boundary and margins\n",
    "    ax.contour(X, Y, P, colors='k',\n",
    "               levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])\n",
    "    \n",
    "    # plot support vectors\n",
    "    if plot_support:\n",
    "        ax.scatter(model.support_vectors_[:, 0],\n",
    "                   model.support_vectors_[:, 1],\n",
    "                   s=300, linewidth=1, facecolors='none');\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fDJ0B563LiIe"
   },
   "outputs": [],
   "source": [
    "# Below only works with 2D features\n",
    "\n",
    "plt.clf\n",
    "plt.figure(figsize=(20, 10)) #facecolor='w'\n",
    "\n",
    "plt.scatter(X_Features_2D.loc[:, 'RE/TA'], X_Features_2D.loc[:, 'ME/TL'], c=Y_Response.loc[:, 'Default'], s=30, cmap='autumn')\n",
    "plot_svc_decision_function(SVM_2D);\n",
    "\n",
    "#plt.xlim([0.0, 1.0])\n",
    "#plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('RE/TA')\n",
    "plt.ylabel('ME/TL')\n",
    "plt.title('Support Vectors: Decision Boundary')\n",
    "#plt.legend(loc=\"lower right\")\n",
    "#plt.savefig('plot_name') #Save into file\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wcy2lIHOLiIg"
   },
   "outputs": [],
   "source": [
    "# Code below plots decision boundaries MANUALLY\n",
    "# This is a prototype BEFORE plot_svc_decision_function() written\n",
    "\n",
    "plt.clf\n",
    "plt.figure(figsize=(20, 10)) #facecolor='w'\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "xlim = (-3, 3)\n",
    "ylim = (-4, 2)\n",
    "\n",
    "# create grid to evaluate model\n",
    "xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "YY, XX = np.meshgrid(yy, xx)\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "\n",
    "Z = SVM_2D.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "# plot decision boundary and margins\n",
    "ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n",
    "           linestyles=['--', '-', '--'])\n",
    "\n",
    "# plot support vectors\n",
    "ax.scatter(SVM_2D.support_vectors_[:, 0], SVM_2D.support_vectors_[:, 1], s=50,\n",
    "           linewidth=1, facecolors='none')\n",
    "\n",
    "# plot support vectors\n",
    "ax.scatter(SVM_2D.support_vectors_[:, 0], SVM_2D.support_vectors_[:, 1])\n",
    "\n",
    "# plot features data\n",
    "ax.scatter(X_Features_2D.loc[:, 'RE/TA'], X_Features_2D.loc[:, 'ME/TL'], c=Y_Response.loc[:, 'Default'], s=30, cmap='autumn')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87qcfYV5LiIi"
   },
   "source": [
    "-----\n",
    "\n",
    "\n",
    "## Chapter. Random Forests and Ensemble Methods (AdaBoost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lBRBsQyaLiIj"
   },
   "outputs": [],
   "source": [
    "# REPEAT of data loading -- for reference\n",
    "#Two features -- essentially Restricted model M0 from Corporate PD.xls formulation.\n",
    "\n",
    "X_Features_2D = X_Features.loc[:,['RE/TA', 'ME/TL']] \n",
    "Y_Response = pd.read_excel(xl, sheet_name='Logit', usecols=[2])\n",
    "Y_Response = Y_Response['Default'].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OVOqm99lLiIl"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7i7PKwhzLiIn"
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "\n",
    "regr_1 = DecisionTreeRegressor(max_depth=4)\n",
    "\n",
    "regr_2 = AdaBoostRegressor(DecisionTreeRegressor(max_depth=4),\n",
    "                          n_estimators=300, random_state=rng)\n",
    "\n",
    "regr_1.fit(X_Features_2D, Y_Response)\n",
    "regr_2.fit(X_Features_2D, Y_Response)\n",
    "\n",
    "# Predict\n",
    "y_1 = regr_1.predict(X_Features_2D)\n",
    "y_2 = regr_2.predict(X_Features_2D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G9YR28GELiIq"
   },
   "outputs": [],
   "source": [
    "import graphviz\n",
    "from sklearn.tree import export_graphviz \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bUWFo1d7LiIs"
   },
   "outputs": [],
   "source": [
    "# Visualising Decision Tree with graphviz\n",
    "\n",
    "dot_data = export_graphviz(regr_1, out_file=None, \n",
    "                           feature_names= ['RE/TA', 'ME/TL'], \n",
    "                           class_names=['survival', 'default'], filled=True,\n",
    "                           rounded=True,\n",
    "                           proportion=True)\n",
    "\n",
    "graph = graphviz.Source(dot_data, filename='../CODE_ML/TreeImageREG_graphviz', format='png')\n",
    "graph.render();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHhn8zd0LiIv"
   },
   "source": [
    "\n",
    "### Decision Tree Visualised (Regressor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XqHT0ZspLiIv"
   },
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state=rng)\n",
    "clf.fit(X_Features_2D, Y_Response)\n",
    "\n",
    "# Predict\n",
    "y_3 = clf.predict(X_Features_2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "avLb5rAXLiIx"
   },
   "outputs": [],
   "source": [
    "# Visualising Decision Tree with graphviz\n",
    "\n",
    "dot_data = export_graphviz(clf, out_file=None, \n",
    "                           feature_names= ['RE/TA', 'ME/TL'], \n",
    "                           class_names=['survival', 'default'], filled=True,\n",
    "                           rounded=True,\n",
    "                           proportion=True)\n",
    "\n",
    "graph = graphviz.Source(dot_data, filename='../CODE_ML/TreeImageCLF_graphviz', format='png')\n",
    "graph.render();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IY0DBc05LiIz"
   },
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_zXmK31oLiIz"
   },
   "outputs": [],
   "source": [
    "plt.clf\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(X_Features_2D['RE/TA'], Y_Response, c=\"k\", label=\"training samples\")\n",
    "\n",
    "plt.scatter(X_Features_2D['RE/TA'], y_1, c=\"g\", label=\"n_estimators=1\", linewidth=2)\n",
    "plt.scatter(X_Features_2D['RE/TA'], y_2, c=\"r\", label=\"n_estimators=300\", linewidth=2)\n",
    "plt.xlabel(\"for feature: Retained Earnings/Total Assets\", fontsize=14)\n",
    "plt.ylabel(\"target\", fontsize=14)\n",
    "plt.title(\"TWO MODELS (Decision Tree - green, Boosted - red): Predictions for Corporate Default\", fontsize=14) #plt.colorbar() ADD ON NEXT RUN\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#xlabel= 'wrt Feature: ' + AltmanScores_names[FeatureBetaIdx][1]\n",
    "#plt.xlabel(xlabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAvm0CAPLiI1"
   },
   "source": [
    "**NOTE** Two models are mixed on scratterplot such as these. In fact there are three datasets: ONE ACTUAL and TWO PREDICTED from our models: \n",
    "\n",
    "```python\n",
    "regr_1 = DecisionTreeRegressor(max_depth=4)\n",
    "\n",
    "regr_2 = AdaBoostRegressor(DecisionTreeRegressor(max_depth=4),\n",
    "                          n_estimators=300, random_state=rng)\n",
    "```\n",
    "\n",
    "\n",
    "The scatterered output reveals plenty of search for our binary classification of default/non-default in Corporate PD dataset.\n",
    "<br><br>\n",
    "\n",
    "The nature of AdaBoost Classifier is to create copies of the trees (meaning copies of the dataset) and we see the search example produced by a random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M9abZWmGLiI2"
   },
   "outputs": [],
   "source": [
    "plt.clf #to clean canvass\n",
    "fig = plt.figure(figsize=(18,10))\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.scatter(X_Features_2D['RE/TA'], Y_Response, c=\"k\", label=\"training samples(data)\")\n",
    "plt.scatter(X_Features_2D['RE/TA'], y_1, c=\"g\", label=\"n_estimators=1\", linewidth=2)\n",
    "#plt.ylabel('Prediction target', fontsize=12) \n",
    "plt.xlabel('Retained Earnings/Total Assets', fontsize=14)\n",
    "plt.title('Decision Tree Regression', fontsize=14)\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.scatter(X_Features_2D['RE/TA'], Y_Response, c=\"k\", label=\"training samples(data)\")\n",
    "plt.scatter(X_Features_2D['RE/TA'], y_2, c=\"r\", label=\"n_estimators=300\", linewidth=2)\n",
    "#plt.ylabel('Prediction target', fontsize=12)\n",
    "plt.xlabel('Retained Earnings/Total Assets', fontsize=14)\n",
    "plt.title('BOOSTED Decision Tree Regression', fontsize=14)\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.scatter(X_Features_2D['RE/TA'], Y_Response, c=\"k\", label=\"training samples(data)\")\n",
    "plt.scatter(X_Features_2D['RE/TA'], y_3, c=\"r\", label=\"n_estimators=300\", linewidth=2)\n",
    "#plt.ylabel('Prediction target', fontsize=12)\n",
    "plt.xlabel('Retained Earnings/Total Assets', fontsize=14)\n",
    "plt.title('Decision Tree CLASSIFIER', fontsize=14)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ONzW2TGLiI5"
   },
   "source": [
    "-----\n",
    "\n",
    "## Radviz from Pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7mtxD2_qLiI5"
   },
   "outputs": [],
   "source": [
    "X_Features_2D = X_Features.loc[:,['RE/TA', 'ME/TL', 'EBIT/TA']] \n",
    "X_Features_2D['Default'] = Y_Response\n",
    "\n",
    "from pandas.plotting import radviz\n",
    "\n",
    "plt.clf\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "radviz(X_Features_2D, 'Default')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "osUV3X6PLiI7"
   },
   "outputs": [],
   "source": [
    "X_Features_2D = X_Features.loc[:,['RE/TA', 'ME/TL', 'S/TA']] \n",
    "X_Features_2D['Default'] = Y_Response\n",
    "\n",
    "from pandas.plotting import radviz\n",
    "\n",
    "plt.clf\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "radviz(X_Features_2D, 'Default')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OF_VN_VeLiI_"
   },
   "source": [
    "------\n",
    "\n",
    "## K-Means Clustering\n",
    "\n",
    "**Below** is an attempt at K-Means Clustering however plotting remains to be fixed. RD\n",
    "\n",
    "Please also see an example in Classifier_Sandbox_IrisData.ipynb that shows KMeans clustering on the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8O517wLLLiI_",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X_Features_KMeans = X_Features.loc[:,['RE/TA', 'ME/TL']] \n",
    "X_Features_KMeans['Default'] = Y_Response\n",
    "\n",
    "#  1) KMeans clustering on the iris dataset\n",
    "n_clusters = 2\n",
    "k_means = KMeans(init='k-means++', n_clusters=n_clusters)\n",
    "k_means.fit(X_Features_KMeans)  # fit the KMeans algo \n",
    "print (k_means.cluster_centers_) # get the cluster centers\n",
    "print (k_means.labels_)  # cluster label for each data point\n",
    "print (np.unique(k_means.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l197uUp0LiJB"
   },
   "outputs": [],
   "source": [
    "# TO FIX the reference inside X_Features_2D \n",
    "#  2) Need to fix plotting of K-mean nearest neighbours\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "featurex = 0  # The first 'feature' in iris.feature_names\n",
    "featurey = 1  # The second 'feature' in iris.feature_names\n",
    "for k in range(n_clusters):\n",
    "    clust_members = k_means.labels_ == k\n",
    "    x = X_Features_KMeans[clust_members, featurex] # CAUSES PROBLEMS\n",
    "    y = X_Features_KMeans[clust_members, featurey] # CAUSES PROBLEMS\n",
    "    ax.plot(x, y, 'o', c=plt.cm.spring(k*100), label=k, markersize=10)\n",
    "    \n",
    "ax.set_xlabel(X_Features_KMeans.feature_names[featurex])\n",
    "ax.set_ylabel(X_Features_KMeans.feature_names[featurey])\n",
    "#  bump the axis values ~+/- 5%\n",
    "ax.set_xlim(np.min(X_Features_KMeans[:, featurex])*0.95, np.max(X_Features_2D[:, featurex])*1.05)\n",
    "ax.set_ylim(np.min(X_Features_KMeans[:, featurey])*0.95, np.max(X_Features_2D[:, featurey])*1.05)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9NLnzslULiJD"
   },
   "source": [
    "\n",
    "------\n",
    "\n",
    "### Common Sklearn Declarations for Classifiers and Crossvalidation in Python\n",
    "\n",
    "The declarations give idea about the depths of functionality. It is important to know that in addition to models, there is **preprocessing** (such as scaling and standardisation) and **model selection/feature selection** by quantaitive metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "15YTNBKbLiJD"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,\n",
    "from sklearn.ensemble import RandomForestRegressor,\n",
    "from sklearn.ensemble import GradientBoostingRegressor,\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "from sklearn import datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMcxk40ELiJG"
   },
   "source": [
    "**END OF CODE FILE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z_JwEkynLiJG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Copy of ML_Lab_Selected CODE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Custom (custom_python)",
   "language": "python",
   "name": "custom_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "nteract": {
   "version": "nteract-on-jupyter@2.0.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
